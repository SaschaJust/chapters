# Gotchas from Mining Bug Reports
 
Over the years, it has become common practice in empirical software engineering to mine data from version archives and bug databases to learn where bugs have been in the past, or to build prediction models to find error prone code in the future. 
Simplistically, one counts the number of fixes per code entity by mapping closed reports about bugs to their corresponding *fixing* code changes--typically one scans for commit messages that mention a bug report identifier. This however relies on three fundamental assumptions:
 
* The location of the defect is the part of the code where the fix is applied.
* Issue reports that are flagged as BUGs describe real code defects.
* The change to the source code is atomic, meaning it's the minimal patch that fixes the bug without performing any other tasks. 
 
Violating the first assumption yields model that predict bug fixes rather than code defects--but there is very little we can do. Violating the second assumption however would lead to noise and bias in our datasets, which effectively leads to counting changes or churn, rather than code defects or bug fixes. Thus, violating the latter two assumption imposes serious risks and might lead to imprecise code quality models.  
 
## What is a bug? A matter of perspective.
 
If we would ask five engineers "What is a bug?" we would probably get six different answers or most of them would answer: "this depends". Asking 10 different people on the street, including people not being software development engineers, would add even more diversity to the answer set. Although this seems unrelated, we should bare in mind that bug reports are communication tools between engineers which may also include customers--people from the street.   
Thus, the threat of violating our second assumption (bug reports describe real code defects) is high, depending on the data source and who is involved in creating bug reports. In nearly all issue tracking systems, the user that creates the bug report is responsible to assign a issue report type: typically something like BUG, RFE (feature request), IMPR (improvement), DOC (documentation), REFAC (refactoring), OTHER (things like backports, cleanups, ...).
However, the user might not understand the difference between these types and thus uses the default type (usually BUG) or may have a different perspective on the issues; users typically see any unexpected behavior as bugs. Still, the user first creating the bug report is the one that decides about the issue category. We illustrate this on a small example between the three parties that are involved in creating and accessing bug report data, i.e. users, engineers and analysts: the *user* of a library wants to access some algorithmic function that is a private property of some entity. She raises a new issue report, having the default category BUG, saying "Cannot access X through Y.". For her, this is clearly a bug, as the program doesn't behave as she expects it to. An *engineer* might support the report and solve her issue. However, he would consider this to be an improvement as this behavior wasn't planned in the first place but seems to be a contribution to the functionality and capability of the library. As an *analyst* mining the data and looking at the actual change, we might be tempted to classify the report and corresponding change as a refactoring, as the engineer only moved the methods to be publically accessible. Even if engineer and analyst would agree on the report category, there is only a low chance that the conflict between report creator and analyst gets resolved. Studies have shown that fields of issue reports change very rarely, once an initial value is assigned. The reason is that engineers have very little benefit of changing these values. Once the engineer knows the actual report category, he is close to a solution of the issue. Going back to the report and changing the report type costs only time. The same is true for  other issue report fields, such as severity or priority.

As a consequence, we as analysts should not blindly rely on user input data, especially if the data might stem from non-experts or data sources that reflect different points of view. We should always keep in mind that the data we analyze is most likely created for purposes other than mining and analyzing the data. Going back to our assumption on bug reports and the types of issue report, we should be careful about simply using the data without checking if our second assumption holds. If it does not hold, it is good practice to estimate the extend of the data noise and whether it will significantly impact our analyses.
 
## Noise and bias in bug data.

To show an example of what could happen if we use noisy data to build defect prediction models, we investigated 7,000 bug reports from five open source projects (HTTPClient, Jackrabbit, Lucene-Java, Rhino and Tomcat5) and manually classified their report types to find out how much noise exists in these frequently used bug report data sets and what impact this noise has on quality models [1].
 
We found that in deed **issue report types are unreliable**. In the five bug databases investigated, more than 40% of issue reports are inaccurately classified. In other words, 2 out of 5 reports had a different type than we as analysts would categorize them for analyses.
Concentrating on bug reports, we discovered that **every third bug is not a bug**. More precisely, in our analysis 33.8% of all BUG reports do not refer to corrective code maintenance at all. In consequence, the validity of studies regarding the distribution and prediction of bugs in code is threatened. Assigning the noisy original bug data to source files to count the number of defects per file, we found that a significant number of **files are wrongly marked as fixed**. More precise, for the five investigated projects, 39% of files originally marked as defective actually *never* had a single bug. Going one step further, the noisy data also affected the defect distribution significantly, as **files are wrongly marked to be error-prone**. We found that 16% to 40% of the top 10% most defect-prone files do not belong in this category after manually reclassification issue report types.

## The impact of tangled changes. 

What about the third assumption commonly made when building quality models based on software repositories: "commits applied to version archives represent atomic code changes"? There have been several studies [1,2] that showed that this assumption is frequently violated. Developers do not always commit atomic code changes, but rather *tangle* multiple development activities, e.g. fixing a bug while writing a new feature, into a single version control commit. The reason for this is similar to the reason of noisy bug report types: there is little to no motivation and benefit for engineers to work this way, e.g. to create local branches to fix simple bugs. An engineer's intention is to complete a development task and to get work done. There is no gain in working in a way data analysts profit from--at least not directly. Although modern version control system's rising popularity in the past years, which makes it  easier to commit single chunks of patches and to create and merge branches (e.g. Git), most engineers still perform multiple tasks at once instead branching for the specific tasks as e.g. proposed in the Git-Flow process. A second reason lies in the fact that some development tasks cannot be split. Often fixing a bug comes with a refactoring or an improvement or even fixing multiple bugs at once. In those cases, reducing the amount of overhead in managing the changes with a version control system does not help much. However, each of these *tangled changes* is threatening the validity of models relying on the assumption that code changes are atomic entities, e.g. assigning the number of bugs by identifying bug references in commit messages and assigning the fixed bug to all files touched in the assumed atomic commit. 

Among other, Herzig and Zeller [3] showed that the bias caused by tangled changes can be significant. In their study, the authors manually classified
more than 7,000 individual change sets and checked whether they address multiple (tangled) issue reports. Their results show that up to 12% of commits are tangled and cause false associations between bug reports and source files.  Further, Herzig [4] showed that tangled changes usually combine two or three development tasks at a time. In the same study Herzig [4] showed also that tangled change sets can severely impact bug counting models. Between
6% and 50% of the most defect prone files do not belong in this category
because they were falsely associated with bug reports. Up to 38% of source files had a false number of bugs associated with them and up to 7% of  files originally associated with bugs never were part of any bug fix.
 
## In summary

Mining bug reports and associating bug fixes to files to assess the quality of the code in these files has become common practice. However, it is crucial to remember that this method relies on assumptions that need to be verified to ensure that resulting models are accurate and model the intended property. There has been outstanding research performed on automatically correcting some of the noise and bias commonly experienced in software repositories, e.g. classifying issue reports using text based approaches [5]. Using manual validation and automated cleanup methods is essential to ensure that the data matches our expectations. Otherwise users of our models will open bug reports complaining about the inaccuracy of our model. As an owner of the model. would you consider that a bug or rather as an improvement?

## References

[1] K. Herzig, S. Just, and A. Zeller, “It’s not a Bug, It’s a Feature: How Misclassification Impacts Bug Prediction,” in Proceedings of the 2013 international conference on software engineering, Piscataway, NJ, USA, 2013, pp. 392-401. 

[2] Kawrykow, D., and Robillard, M. P. "Non-essential changes in version histories." In Proceedings of the 33rd International Conference on Software Engineering (New York, NY, USA, 2011), ICSE ’11, ACM, pp. 351–360.

[3] K. Herzig and A. Zeller, “The Impact of Tangled Code Changes,” in Proceedings of the 10th working conference on mining software repositories, Piscataway, NJ, USA, 2013, pp. 121-130. 

[4] K. Herzig, “Mining and Untangling Change Genealogies,” PhD Thesis, 2012. 

[5] Antoniol et al., "Is it a bug or an enhancement? A text-based approach to classify change requests" In Proceedings of the 2008 conference of the center for advanced studies on collaborative research: meeting of minds (New York, NY, USA, 2008), CASCON’08, ACM, pp. 23:304–23:318.
