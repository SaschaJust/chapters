Identifying Fault-prone Files in Large Industrial Software Systems
==================================================================

Elaine Weyuker
--------------
Tom Ostrand 
-----------

A person walks down the street and sees a man on his hands and knees under a lamppost. 

“What are you doing?”
“I’m looking for my keys.” 
“You dropped them here?” 
“No, but the light is so much better here than where I dropped them, I thought I’d look here!”

A silly old joke, but one that motivated the work that we did on software fault prediction for almost a decade while working in industry . 

Testing is critical and expensive and what every tester would love to be able to do is focus their lampposts exactly where the bugs are so that they could test the relevant program parts most intensively. If only we knew beforehand where the bugs were likely to be, we could test there first, test there hardest, assign our best testers, allocate sufficient time, etc.. In short, we could use that information to prioritize our testing efforts to find bugs faster and more efficiently, leading to more reliable software. That is exactly what our software fault prediction algorithms aim to do - identify those parts of a software system that are most likely to contain the largest numbers of bugs.

To help achieve this goal, we developed algorithms which use static code characteristics such as file size and programming language, and historical data such as the number of recent bugs and the number of recent changes made to each file, to predict the number of bugs that will be in each file in the next release of the system. But first we had to determine which were the most appropriate file characteristics to use, and how best to weight those characteristics. To create effective algorithms, we needed to identify which file characteristics correlated most closely with files containing the largest numbers of bugs. As the basis for our research, we looked for real software projects with multiple releases, comprehensive version history and an accessible bug database that we could use to first identify the file characteristics and their relation to bugs. We needed to work with practitioners and first get their interest in the project so that they would give us access to their software and data. As we studied their systems and built our first prediction models, we needed to continue working with practitioners to make sure we were asking the right questions and providing feedback in a form that would be useful to them.

We first looked at an inventory control system containing roughly a half million lines of code that had been in the field for about 3 years. We had data for 12 releases. Like most systems at AT&T, the software had been developed using a traditional waterfall model, and had four releases a year (quarterly releases). The system used a proprietary configuration management system that included both version control and change management functionality. Underlying the configuration management system was a very large database that contained all of the information about the system, including the source code.

We identified the most buggy files in each release and looked at what they had in common in order to identify file characteristics that correlated with fault-proneness. We started with characteristics that our intuition, experience, and the folklore told us were most important. For example, everyone knows that “big is bad” - that has been a defining mantra of the Software Engineering world for the last few decades and so we looked at file size as a potential characteristic, and found that it did indeed correlate with fault-proneness. Another common belief is “once buggy, always buggy”; files that had bugs in the past are likely to have bugs later. We found, in fact, that while past bugginess was indeed important, it was only the most recent release’s status that really mattered. We continued looking at other characteristics we expected to be most important. While we had intuition about which file characteristics would be most important, it was necessary to validate that intuition by doing empirical studies to confirm their importance. Eventually we determined that by mining the database associated with the configuration management system, five simple file characteristics were sufficient to allow us to build a model that made accurate predictions: file size, the number of changes in the two previous releases, the number of bugs in the most recent previous release, the age of the file in terms of the number of releases the file has been part of the system, and the language in which the file was written. 

It took us a number of tries to get a predictive model that seemed to make these predictions accurately. We tried several methods of using the data to make predictions, and eventually decided to use the statistical method of negative binomial regression. For the initial system, we found that, averaged over the 17 releases of this system, the files predicted to have the largest numbers of bugs did indeed contain a large majority of the bugs. To assess the effectiveness of the predictions, for each release we measured the percent of all real bugs that turned out to be located in the 20% of files that were predicted to have the largest numbers of bugs. Over the system's 17 releases, the percent of bugs actually detected in the predicted "worst 20%" of files averaged 83%. Armed with these promising results we were able to attract other projects whose software we could study and for which we could make predictions.

We refined and improved the prediction model while working with three additional projects, eventually settling on a Standard Model that uses the five characteristics mentioned above.  The basic ideas and method to calculate the predictions are explained in [1], although that paper describes an early version of the prediction model.

Because easy access to the prediction technology is just as important as the technology itself, we built a tool that provides testers and developers with a simple GUI interface to input the information needed to request bug prediction for their system. Users identify their system, the types of files they are interested in (Java, C, C++, SQL, etc.), and the release they want predictions for.  The tool does the calculations and presents a list of the release's files sorted in decreasing order of the number of predicted bugs.  The results can be used to prioritize the order and intensity of testing files, to assign the most appropriate tester(s) to specific files or parts of the system, to help decide which files should be revised or completely rewritten, and to decide whether other quality assurance procedures such as a detailed code review should be carried out. Because a typical run of the tool is very quick, often under 1 minute even for a multi-million line system, users could run it repeatedly to get different views, perhaps to restrict their interest to just Java files, or to a subset of the entire system, or to find out if a refactoring of the code changes the expected fault likelihood.

Over the course of several years we studied a total of nine projects and made predictions for a total of 170 releases. All nine systems were in the field for multiple years, all ran continuously (24/7) and ranged from a system with just nine releases over the course of two years, to two systems that were in the field for almost ten years each, with each having 35 quarterly releases. The systems performed all sorts of different tasks, were written in different languages, and ranged in size from under 300,000 lines of code to over 2,100,000 lines of code. The prediction accuracy ranged from 75% to 93%. 

An unexpected episode at AT&T gave us even more confidence in the usefulness of the prediction models.   At a meeting attended by several development managers, we demonstrated the prediction model, and showed the results we had obtained for the last released version of one of the major systems.  After seeing this, that system’s manager asked whether we could run the model on the version that was currently under development and next in line to be released.  On the spot, we were able to access the system’s version control and bug database, and generate predictions.  The results were highly useful to the manager, as they provided fresh insight to his system. The files that were predicted to be most faulty included several that the manager already knew had potential weaknesses, but also included several that he had not considered problematic.  His first action on leaving the meeting was to instruct his testing team to do intensified testing on the files that were previously considered safe.

The Standard Model does not account for some variables that many people (including us!) felt might have an effect on the potential bugginess of code.  We experimented with augmented models that included counts of the number of different programmers who had changed the code [2], the complexity of the code's calling structure, and detailed counts of the number of lines added, changed or deleted in a file.  None of these additional characteristics significantly improved the Standard Model's predictions, and we even found that adding additional variables sometimes made the predictions worse. 

Software fault prediction has been investigated frequently over the past 20 years, with mixed results. Our work demonstrates that at least for the types of systems and development processes we studied, accurate prediction results are possible, and can provide useful information to testers, developers, and project managers. 

1. T.J. Ostrand, E.J. Weyuker, R.M. Bell, Predicting the Location and Number of Faults in Large Software Systems, *IEEE Trans. on Software Engineering*, Vol 31, April 2005, 

2. E.J. Weyuker, T.J. Ostrand, R.M. Bell, Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models, *Empirical Software Engineering*, Vol 13, 2008, pp. 539-559
