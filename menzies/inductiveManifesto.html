<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="seven-principles-of-inductive-software-engineering-what-we-do-is-different.">Seven Principles of Inductive Software Engineering: What we do IS Different.</h1>
<p><em>Tim Menzies</em></p>
<h2 id="different-and-important">Different and Important</h2>
<p><em>Inductive software engineering</em> is that branch of software engineering focusing on the delivery of data mining based software applications to users. Within those data miners, the core problem is <em>induction</em> which is the extraction of small patterns from larger data sets. Inductive engineers spend much effort trying to understand user goals in order to inductively generate the models that most matter to those user.</p>
<p>Previously, with Christian Bird, Thomas Zimmermann, Wolfram Schulte, and Ekrem Kocaganeli, we wrote an <em>Inductive Engineering Manifesto</em> [1] that offered some details on this new kind of engineering. That whole manifesto is a little long, so here I offer a quick summary. Listed below are seven key principles which, if ignored, can make it harder to deploya analytics in the real world. For more details (and more principles), the reader should refer to the original document [1].</p>
<h2 id="principle-1-users-before-algorithms">Principle #1: Users Before Algorithms</h2>
<p>Mining algorithms are only good if users find their use in real-world applications. This means that they need to</p>
<ul>
<li>understand the results</li>
<li>understand that those results add value to their work.</li>
</ul>
<p>Accordingly, it is strongly recommend that once the algorithms generate some model, then the inductive engineer <em>talks to the users</em> about those results.</p>
<p>In our experience, such discussions lead to a second, third, fourth, etc round of learning. To assess if you are talking in &quot;the right way&quot; to your users, check the following:</p>
<ul>
<li>Do users bring their senior management to the meetings? If yes, great!</li>
<li>Do users keep interrupting (you or each other) and debating your results? If yes, then shut the hell up (and take lots notes!)</li>
<li>Do the users indicate they understand your explanation of the results? E.g. can they correctly extend your results to list desirable, undesirable implications of your results?</li>
<li>Do your results are touching on issues that concern them? This is <em>easy</em> to check.... just count how many times they glance up from their notes, looking startled or alarmed.</li>
<li>Do your user offer more data sources for analysis? If yes, they like what you are doing and want you to do it more.</li>
<li>Do users invite you to their workspace and ask you to teach them how to do XYZ? If yes, this is a real win. Ideally, inductive software engineers build a team of engineers and users, each of which can contribute significantly to the process.</li>
</ul>
<h2 id="principle-2-plan-for-scale">Principle #2: Plan for Scale</h2>
<p>Data mining methods are usually repeated multiple times in order to:</p>
<ul>
<li>Answer new user questions, inspired by the current results;</li>
<li>Enhance data mining method or fix some bugs;</li>
<li>Deploy the results, or the analysis methods, to different user groups</li>
</ul>
<p>So that means that, if it works, you will be asked to do it again (and again (and again (and again))). To put that another way <em>thou shalt not click</em>. That is, if all your analysis requires lots of point-and-clock in a pretty GUI environment, then you are definitely <strong>not</strong> planning for scale.</p>
<p>Another issues is that as you scale up, your methods will need to scale up as well. For example, in our <em>Manifesto</em> document, we discussed the CRANE project at Microsoft that deployed data mining methods to the code base of Microsoft Windows. This was a <em>very large</em> project so the way it started was <em>not</em> the way it ended:</p>
<ul>
<li>Initially, a single inductive engineer did some rapid prototyping for a few week, to explore a range of hypotheses and gain user interest (and get user feedback on the early results);</li>
<li>Next, the inductive engineering team spent a few months conducted many experiments to find stable models (and to narrow in on the most important user goals).</li>
<li>In the final stage, which took a year, the inductive engineers integrate the models into a deployment framework that was suitable for target user base</li>
</ul>
<p>Note that the team size doubled at each stage-- so anyone funding this works needs to know that increasingly useful conclusions can be increasingly expensive.</p>
<h2 id="principle-3-get-early-feedback">Principle #3: Get Early Feedback</h2>
<p>This mentioned this above, but it is worth repeating. Before conducting very elaborate studies (that take a long time to reach a conclusion), try applying very simple tools to gain rapid early feedback</p>
<p>So simplicity first! Get feedback early and often! For example, there are many linear time discretizers for learning what are good divisions of continuous attributes (e.g. the Fayyad-Irani discretizer [2]). These methods can also report when breaking up an attribute is <em>not</em> useful since that attribute is not very informative. Using tools like these, it is possible to discover what attributes can be safely ignored (hint: usually, its more than half).</p>
<h2 id="principle-4-be-open-minded">Principle #4: Be Open-Minded</h2>
<p>The goal of inductive engineering for SE is to find better ideas than what were available when you started. So if you leave a data mining projects with the same beliefs as when you've started, you really wasted a lot of time and effort. Hence, some mantras to chant while data mining are:</p>
<ul>
<li>Avoid a fixed hypothesis. Be respectful but doubtful to all user-suggested domain hypotheses. Certainly, explore the issues that they raise but also take the time to look further afield.</li>
<li>Avoid a fixed approach for data mining (e.g. just using decision trees all the time), particularly for data not been mined before</li>
<li>The most important initial results are the ones that radically and dramatically improve the goals of the project. So seek important results.</li>
</ul>
<h2 id="principle-5-be-smart-with-your-learning">Principle #5: Be Smart with your Learning</h2>
<p>Lets face it, any inductive agents (human or otherwise) has biases that can confuse the learning process. So donâ€™t torture the data to meet preconceptions (that is, it is ok to go &quot;fishing&quot; to look for new insights).</p>
<p>It also true that any inductive agent (and this includes you) can make mistakes. If organizations are going to use your results to change policy, the important outcomes are riding on your conclusions. This means you need to check and validate your results:</p>
<ul>
<li><p>Ask other people to do code reviews of your scripts.</p></li>
<li>Check conclusion stability against different samples policies. For example:</li>
<li>Policy1: divide data into (say) ten 90% samples (built at random). How much do your conclusions change across those samples?</li>
<li>Policy2: sort data by collection date (if available). Learn from the far fast, the nearer past, then most recent data. Does time change your results? Is time <em>still</em> changing your results? It is important to check.</li>
<li>Policy3,4,5,etc: Are there any other natural divisions of your data (e.g. east coast vs west coast; men vs women; etc)? Do they effect your conclusions?</li>
<li>When reporting multiple runs of a learner, don't just report mean results-- also report the wriggle around the mean.</li>
<li><p>In fact, do not report mean results at all since outliers can distort those mean values. Instead, try to report median and IQR results (the inter-quartile range is the difference between the 75th and 25th percentile).</p></li>
</ul>
<h2 id="principle-6-live-with-the-data-you-have">Principle #6: Live with the data you have</h2>
<p>In practice, it is a rare analaytics projec that can dictate how data is collected in industrial contexts. Usually, inductive engieners have to cope with whatever data is available (rather than demand more data collected under more ideal conditions). Ths means that often you have to go mining with the data you have (and not the data you hope to have a some later date). So its important to spend some time on data quality operators. For example:</p>
<ul>
<li>Use <em>feature selection</em> to remove spurious attributes. There are many ways to perform such feature selection including the Fayyad Irani method discussed above. For a discussion of other feature selection methods, see [3].</li>
<li>Use <em>row selection</em> to remove outliers and group related rows into sets of clusters. For a discussion on row selection methods, see [4]</li>
</ul>
<p>One benefit of replaces rows with clusters is that any signal that is spread out amongst the rows can be &quot;amplified&quot; in the clusters. Ifwe cluster, then learn one model per cluster, then the resulting predictions have better median values and smaller variance [5].</p>
<p>In any case, we've often found row and feature selection discards up to 80 to 90% of the data, without damaging our ability to learn from the data. This means that ensuring quality of <em>all</em> the data can sometimes be less important that being able to extract quality data from large examples.</p>
<h2 id="principle-7-develop-a-broad-skill-set-that-uses-a-big-toolkit.">Principle #7: Develop a Broad Skill Set that uses a Big Toolkit.</h2>
<p>The reason organizations need to hire inductive engineers is that they come equiped with a very broad range of tools. This is important since many problems need specialized methods to find good solutions.</p>
<p>So, to become an inductive engineer, look for the &quot;big ecology&quot; toolkits where lots of developers are constantly trying out new ideas. Languages like Python, Scala (and lately, Julia) have extensive on-line forums were developers share their data mining tips. Toolkits like R, MATLAB, WEKA are continually being updated with new tools.</p>
<p>What a great time to be an inductive engineer! So much to learn, so much available to try. Would you want to have it any other way?</p>
<h2 id="references">References</h2>
<ol style="list-style-type: decimal">
<li>Tim Menzies, Christian Bird, Thomas Zimmermann, Wolfram Schulte, and Ekrem Kocaganeli. 2011. The inductive software engineering manifesto: principles for industrial data mining. In Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering (MALETS '11). ACM, New York, NY, USA, 19-26. DOI=http://dx.doi.org/10.1145/2070821.2070824</li>
<li>Usama M. Fayyad and Keki B. Irani. 1992. On the Handling of Continuous-Valued Attributes in Decision Tree Generation. Mach. Learn. 8, 1 (January 1992), 87-102. DOI=http://dx.doi.org/10.1023/A:1022638503176</li>
<li>Mark A. Hall and Geoffrey Holmes. 2003. Benchmarking Attribute Selection Techniques for Discrete Class Data Mining. IEEE Trans. on Knowl. and Data Eng. 15, 6 (November 2003), 1437-1447. DOI=http://dx.doi.org/10.1109/TKDE.2003.1245283</li>
<li>Olvera-LÃ³pez, J.Arturo; Carrasco-Ochoa, J.Ariel; MartÃ­nez-Trinidad, J.Francisco; Kittler, Josef. 2010. A review of instance selection methods Artificial Intelligence Review 34(2) pages 133-143 DOI=http://dx.doi.org/10.1007/s10462-010-9165-y</li>
<li>Menzies, T.; Butcher, A.; Cok, D.; Marcus, A.; Layman, L.; Shull, F.; Turhan, B.; Zimmermann, T., &quot;Local versus Global Lessons for Defect Prediction and Effort Estimation,&quot; in Software Engineering, IEEE Transactions on , vol.39, no.6, pp.822-834, June 2013 doi: http://dx.doi.org/10.1109/TSE.2012.83</li>
</ol>
</body>
</html>
