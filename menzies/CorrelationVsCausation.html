<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="correlation-is-not-causation-or-when-not-to-scream-eureka">Correlation is not Causation (or, when not to scream &quot;Eureka!&quot;)</h1>
<h2 id="what-not-to-do">What not to do</h2>
<p>Legend has it that Archimedes once solved a problem sitting in his bathtub. Crying <em>Eureka!</em> (&quot;I have it!&quot;), Archimedes leapt out of the bath and ran to tell the king about the solution. Legend does not say if he stopped to get dressed first.</p>
<p>When we stumble onto some pattern in the data, it is so tempting to send a <em>Eureka!</em> text to the business users. This is a natural response that stems from the excitement of doing science and discovering an effect that no one has ever seen before.</p>
<p>Here's my warning: don't do it. At least, don't do it straight away.</p>
<p>I say this because I have often fallen into the trap of <em>correlation is not causation</em>. Which is to say, just because some connection pattern has been observed between variables does not necessarily imply that a real-world causal mechanism has been discovered. In fact, that &quot;pattern&quot; may actually just be an accident- a mere quirk of cosmic randomness.</p>
<h2 id="example">Example</h2>
<p>For an example of nature tricking us and offering a &quot;pattern&quot; where, in fact, no such pattern exists, consider the following two squares (this example comes from Peter Norvig). One of these was generated by people pretending to be a coin toss while the others were generated by actually tossing a coin, then writing vertical and horizontal marks for heads or tails.</p>
<pre><code>-||--|-|-||-|-||-||-|--|-|
--||---|--||--|-|--|-|-|--
---|-|-|--||-|-|||-|--|-||
--|-|-||--|--||-||-|-|-||-
-|-||--||-||-||-|-|--|-|||
|-||||-||-|||-|-|||-||---|
|-|-|-||--|--|---|-|--||-|
-|-|||--|-||-||-|-|-||---|
-|--||----|||-|-||-|-||-|-
||-|||-|-|-||-|--|-|-||||-
---||-|-|||--|-|-|---|-|--
|||--|--|-|-||-||-|-|-||-|
           (A)
           
-|-|||-----|-------||--|-
-||--|||||--|--|-|||-||||
--||----||-||-|----|--|-|
||-|-|-|||-||--|||-|-||||
|-|||-|-|--||-|-|-||--|--
||-|--|-----|----|---||--
||---|---|-||||-|||||-|-|
|---|---||-||||-|-|------
-|---|-|||-|---||-||-|---
|||-||----||||||-|||||---
|-|------||----||-||-----
-|||-|||-|--|--|-||------
             (B)</code></pre>
<p>Can you tell which one is really random? Clearly, not (B) since it has too many long runs of horizontal and vertical marks. But hang on-- is that true? If we toss a coin 300 times, then at probability 1/4, 1/8, 1/16, 1/32 we will get a run of the same mark that is three, four, five, or six ticks long (respectively). Now 1/32*300=9 so in (B), we might expect several runs that are at least six ticks long. That is, these &quot;patterns&quot; of long ticks in (B) are actually just random noise.</p>
<h2 id="examples-from-software-engineering">Examples from Software Engineering</h2>
<p>Sadly, there are many examples in SE of data scientists uncovering &quot;patterns&quot; which, in retrospect, was more &quot;jumping at shadows&quot; than discovering some underlying causal mechanism. For example, Shull et al. reported one study at NASA's Software Engineering Laboratory that &quot;discovered&quot; a category of software that seemed inherently most bug prone. The problem with that conclusion was that, while certainly true, it missed an important factor. It turns out that that particular sub-system was the one deemed least critical by NASA. Hence, it was standard policy to let newcomers work on that sub-system in order to learn the domain. Since such beginners make more mistakes, then it is hardly surprising that this particular sub-system saw most errors.</p>
<p>For another example, Kocaguneli et al. had to determine which code files were created by a distributed or centralized development process. This, in turn, meant mapping files to their authors, and then situating some author in a particular building in a particular city and country. After weeks of work they &quot;discovered&quot; that a very small number of people seemed to have produced most of the core changes to certain Microsoft products. Note that if this was the reality of work at Microsoft, it would mean that product quality would be most assured by focusing more on this small core group of programmers.</p>
<p>However, that conclusion was completely wrong. Microsoft is a highly optimized organization that takes full advantage of the benefits of auto-generated code. That generation occurs when software binaries are being built and, at Microsoft, that build process is controlled by a small number of skilled engineers. As a result, most of the files appeared to be &quot;owned&quot; by these build engineers even though these files are built from code provided by a very large number of programmers working across the Microsoft organization. Hence, Kocaguneli had to look elsewhere for methods to improve productivity at Microsoft.</p>
<h2 id="what-to-do">What to do</h2>
<p>Much has been written on how to avoid spurious and misleading correlations to lead to bogus &quot;discoveries&quot;. Vic Basili and Steve Easterbrook and colleagues advocate a &quot;top-down&quot; approach to data analysis where the collection process is controlled by research questions, and where those questions are defined <em>before</em> data collection.</p>
<p>The advantage of &quot;top-down&quot; is that you never ask data &quot;what have you got?&quot;-- a question that can lead to the &quot;discovery&quot; of bogus patterns. Instead, you only ask &quot;have you got X?&quot; where &quot;X&quot; was defined before the data was collected.</p>
<p>In practice, there are many issues with top-down, not the least of which is that in SE data analytics, we are often processing data that was collected for some other purpose than our current investigation. And when we cannot control data collection, we often have to ask the open-ended question&quot;what is there?&quot; rather than the top-down question of &quot;is X there?&quot;.</p>
<p>In practice, it may be best to mix up top-down with some &quot;look around&quot; inquires:</p>
<ul>
<li>Normally, before we look at the data, there are questions we think are important and issues we want to explore.</li>
<li>After contact with the data, we might find that other issues are actually more important and that other questions might be more relevant and answerable.</li>
</ul>
<p>In defense of a little less top-down analysis, I note that many important accidental discoveries might have been overlooked if researchers restricted themselves to just the questions defined before data collection. Here is a list of discoveries, all made by researchers pursuing other goals:</p>
<ul>
<li>North America (by Columbus)</li>
<li>Penicillin</li>
<li>Radiation from the big bang;</li>
<li>Cardiac pacemakers (the first pacemaker was a badly built cardiac monitor);</li>
<li>X-ray photography;</li>
<li>Insulin;</li>
<li>Microwave ovens;</li>
<li>Velcro;</li>
<li>Teflon;</li>
<li>Vulcanized rubber;</li>
<li>Viagra.</li>
</ul>
<h2 id="in-summary-wait-and-reflect-before-you-report">In Summary: Wait and Reflect Before you Report</h2>
<p>My message is <em>not</em> that data miners are useless algorithms that torture data till they surrender some spurious conclusion. By asking open-ended &quot;what can you see?&quot; questions, our data miners can find unexpected novel patterns that are actually true and useful-- even if those patterns fly in the face of accepted wisdom. For example, Schmidt and Lipson's Eureqa machine can learn models that make no sense (with respect to current theories of biology) yet can make accurate predictions on complex phenomena (e.g.<br />ion exchanges between living cells).</p>
<p>But, while data miners can actually produce useful models, sometimes they make mistakes. So, my advice is:</p>
<ul>
<li>Always, always, always, wait a few days.</li>
<li>Most definitely, <strong>do not</strong> confuse business users with such recent raw results.</li>
</ul>
<p>In summary. do not rush to report the conclusions that you just uncovered, just this morning. For example, in the case of the Kocaguneli et al. study, if a little more time had been taken reading the raw data, then they would have found the files written by the core group all had funny auto-generated names (e.g. &quot;S0001.h&quot;). This would have been a big clue that something funny was happenning here.</p>
<p>And while you wait, critically and carefully review how you reached that result. See if you can reproduce it using other tools and techniques or, at the very least, implement your analysis a second time using the same tools (just to check if the first result came from some one letter typo in your scripts).</p>
<h2 id="references">References</h2>
<ul>
<li>Victor R. Basili. 1992. Software Modeling and Measurement: The Goal/Question/Metric Paradigm. Technical Report. University of Maryland at College Park, College Park, MD, USA.</li>
<li>Easterbrook, Steve; Singer, Janice; Storey, Margaret-Anne; Damian, Daniela; Selecting empirical methods for software engineering research Guide to advanced empirical software engineering 285-311 2008 Springer London</li>
<li>Ekrem Kocaguneli, Thomas Zimmermann, Christian Bird, Nachiappan Nagappan, and Tim Menzies. 2013. Distributed development considered harmful?. In Proceedings of the 2013 International Conference on Software Engineering (ICSE '13). IEEE Press, Piscataway, NJ, USA, 882-890.</li>
<li>Peter Norving, Warning Signs in Experimental Design and Interpretation, http://goo.gl/x0rI2</li>
<li>Schmidt M., Lipson H. (2009) Distilling Free-Form Natural Laws from Experimental Data, Science, Vol. 324, no. 5923, pp. 81 - 85.<br /></li>
<li>Forrest Shull, Manoel G. Mendoncaa, Victor Basili, Jeffrey Carver, Jose; C. Maldonado, Sandra Fabbri, Guilherme Horta Travassos, and Maria Cristina Ferreira. 2004. Knowledge-Sharing Issues in Experimental Software Engineering. Empirical Softw. Engg. 9, 1-2 (March 2004), 111-137.</li>
</ul>
</body>
</html>
